\documentclass[a4paper, fleqn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{lastpage}
\usepackage{tikz}
\usepackage{float}
\usepackage{textcomp}
\usetikzlibrary{patterns}
\usepackage{pdfpages}
\usepackage{gauss}
\usepackage{fancyvrb}
\usepackage[table]{colortbl}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage[margin=2.5 cm]{geometry}

\setlength\parindent{0pt}
\setlength\mathindent{75pt}

\definecolor{listinggray}{gray}{0.9}
\usepackage{listings}
\lstset{
	language=,
	literate=
		{æ}{{\ae}}1
		{ø}{{\o}}1
		{å}{{\aa}}1
		{Æ}{{\AE}}1
		{Ø}{{\O}}1
		{Å}{{\AA}}1,
	backgroundcolor=\color{listinggray},
	tabsize=3,
	rulecolor=,
	basicstyle=\scriptsize,
	upquote=true,
	aboveskip={0.2\baselineskip},
	columns=fixed,
	showstringspaces=false,
	extendedchars=true,
	breaklines=true,
	prebreak =\raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	frame=single,
	showtabs=false,
	showspaces=false,
	showlines=true,
	showstringspaces=false,
	identifierstyle=\ttfamily,
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941},
  moredelim=**[is][\color{blue}]{@}{@},
}

\lstdefinestyle{base}{
  emptylines=1,
  breaklines=true,
  basicstyle=\ttfamily\color{black},
}

\pagestyle{fancy}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\newcommand*\squared[1]{%
  \tikz[baseline=(R.base)]\node[draw,rectangle,inner sep=0.5pt](R) {#1};\!}
\newcommand{\comment}[1]{%
  \text{\phantom{(#1)}} \tag{#1}}
\newcommand\vgap{\noalign{\vskip 0.1cm}}
\def\el{[\![}
\def\er{]\!]}
\def\dpip{|\!|}
\def\MeanN{\frac{1}{N}\sum^N_{n=1}}
\cfoot{Page \thepage\ of \pageref{LastPage}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\author{Nikolaj Dybdahl Rathcke (Student ID: 74763954)}
\title{Optimization - MATH412 \\ Assignment 1}
\lhead{Optimization - MATH412}
\rhead{Assignment 1}

\begin{document}

\maketitle

\section{Question 1}
We want to spend at most $M$ dollar on the shares $i$ so we maximize $\sum_i x_ir_i$. The optimization problem is subject to three things:
\begin{itemize}
  \item We can't spend more than $M$ dollars on $\sum_i x_i$
  \item We have to ensure we don't exceed the maximal risk, that is $\sum_i \sum_j \sigma_{ij}x_ix_j$ cannot be larger than a given maximal risk $R$.
  \item We can't spend a negative amount on any $x_i$
\end{itemize}
Thus, we formulate our problem as follows:
\begin{equation}
  \begin{array}{rrrl}
    &\text{Maximize:}   & \sum_i x_ir_i  & \\
    \vgap
    \hline
    \vgap
    &\text{Subject to:} & \sum_i x_i & \leq M \\
    &                   & \sum_i \sum_j \sigma_{ij}x_ix_j &\leq R \\
    &                   & \forall_i\text{,}\ \ x_i & \geq 0
  \end{array}
\end{equation}
Where the constraints are in the same order as they were listed above.

\section{Question 2}
We fixed things (so not subject to constraints) are the mast's length $L$, the mast's mass $m$, the points $(x_i, y_i)$ where the wires are attached to, the length of the wires $\ell_i$ and some $k$ to calculate the energies of the tension in the wires. If I understand gravitational potential energy correctly, it is $mgh/2$, where $h$ is the $z$-position for the top of the mast meaning $h/2$ is the center of mass for the mast (and $g$ the gravitational acceleration constant). The energy in the tension of a wire located in $(x_i,y_i)$ is:
\begin{align*}
  [t_i]_+ &= \max\left\{ \frac{k}{2}\left( \sqrt{(x-x_i)^2+(y-y_i)^2+z^2}-\ell_i\right)^2, 0\right\}
\end{align*}
The constraints we need to take into account are:
\begin{itemize}
  \item The top of the mast is actually length $L$ away from the origin.
  \item The position $z$ is positive (mast cannot be underground).
\end{itemize}
Which is all as there are no constraints for the $x$ and $y$ coordinates. So we can form the following constrained optimization (finding $(x,y,z)$) problem:
\begin{equation}
  \begin{array}{rrrl}
    &\text{Minimize:}   & mgz/2 + \sum_i [t_i]_+ & \\
    \vgap
    \hline
    \vgap
    &\text{Subject to:} & \sqrt{x^2+y^2+z^2} & = L \\
    &                   & z & \geq 0
  \end{array}
\end{equation}
Again, the constraints are in the same order as they are listed above.

\section{Question 3}
\subsection{(a)}
Since the optimal function value for Rosenbrock's function is $0$, we look at the ratios $f(x_k)/f(x_{k-1})$. If we play around a little with the print function found in \texttt{math412demo.m}, we can output the ratio along with the function values for all $21$ iterations (accuracy is $10^{-5}$):
\begin{verbatim}
       itn      f value        ratio
         1      4.73195
         2      4.08778       0.8639
         3      3.22853       0.7898
         4      3.22371       0.9985
         5      1.94126       0.6022
         6       1.5991       0.8237
         7      1.17655       0.7358
         8     0.927681       0.7885
         9     0.595927       0.6424
        10     0.452672       0.7596
        11     0.278192       0.6146
        12     0.229962       0.8266
        13    0.0854776       0.3717
        14    0.0494553       0.5786
        15    0.0180854       0.3657
        16   0.00727567       0.4023
        17  0.000782996       0.1076
        18  4.79174e-05       0.0612
        19  5.23502e-08       0.0011
        20  2.62819e-13   5.0204e-06
        21  6.85281e-19   2.6074e-06
\end{verbatim}
The function value is decreasing, but the convergence rate is not monotonically decreasing, though we can see that it has that tendency. Now we know it has quadratic convergence, but from the output it is hard to say something about the convergence rate from the early steps, though the fact it has a tendency to decrease suggests it is quadratic or at least superlinear. \\
The convergence in the final iterations is extremely fast. This is probably due to the fact that it needs to find the "valley" in the function. With Newton's method, when it is within a certain range of a local minimum, usually called a neighbourhood $N$, the convergence is extremely fast, which is most likely what happened around iteration $17$.

\subsection{(b)}
Lets look at the convergence rates for steepest descent. We use the same settings as we had for Newton's method. Since we hit the maximum number of iterations when we use steepest descent, we did not actually find a solution, but here is the info on the last $4$ iterations:
\begin{verbatim}
       itn      f value    ratio
       347   0.00259442   0.9981
       348   0.00259228   0.9992
       349   0.00258512   0.9972
       350   0.00257896   0.9976
\end{verbatim}
So as we can see, the convergence is really slow which suggests (or shows) that the convergence is linear. Even if we increased the number of iteration, we would get the same convergence rate. \\
As mentioned in (a), in the final $4$ iterations, Newton's method convergence very fast (quadratic) towards a solution because it is in the neighbourhood $N$. \\
Comparing the two, for this particular function, Newton's method performed a lot better. Even though Newton's method suffers from the problem that it is not guaranteed to find a solution, when we have a good guess for a starting point, it is often preferable to use.

\subsection{(c)}
For the Newton method, the $\alpha$-value is pretty much $1$ for each iteration, but it is smaller in some of the iteration in the beginning and middle. But for the last $7$ iterations it is $1$. Remember when $\alpha_k=1$, it means the method converges faster towards a solution, which again makes sense as it has located the valley where the minimum is. \\
For steepest descent, the $\alpha$-values fluctuates between $0.19$ and $0.39$. It seems like it is $0.39$ every fifth iteration and $0.19$ otherwise. It makes sense that the value is rather low as we have observed it converges slower (it did not find a solution).

\subsection{(d)}
Using $1000$ iterations with steepest descent does not yield a solution either. If we look at the function value, it almost does not change, so we have a very slow convergence. This is the same problem that we have in the handout, where no perceptible change in the objective function either.

\subsection{(e)}
The two methods runs in exactly the same way. This is because the Hessian is positive definite in all iterations. This makes the methods similar as the implemented Newton method in \texttt{math412demo.m} will use steepest descent if it not positive definite, while the modified version will add a multiple of the unit matrix to make it positive definite in order to use it to hopefully provide a better convergence than the steepest descent method.

\section{Question 4}
Using Newton's method with parameter "BA" finds a solution in $81$ iterations with accuracy $10^{-5}$. Up until iteration $50$, we have that $\mu<0$, meaning the Hessian is not positive definite and doesn't provide us with much information. The $\alpha$-values are $1$ most of the time, but for the first $49$ iteration it is solely $1$'s and afterwards we start getting lower values for $\alpha$. If we look at the convergence rate (I'm leaving the output as it is very long), we see a faster convergence in the beginning where we are actually using steepest descent, but in the end it is much slower. This is probably because of the "shallow dome" the function has. It means the function value doesn't change much compared to the valley found in Rosenbrock's function. \\
Using the modified Newton method with parameter "BA", we see the same kind of convergence rate and values for $\alpha$. However, up until iteration $50$, we have $\mu = 1$ (because we use the modified version), meaning the Hessian is made positive definite and we can run a successful iteration of the modified Newton method which can give us better convergence. However, it does not seem to be the case that it actually gives a better convergence than the steepest descent method. \\
\\
Running Newton's method with "AF" instead finds a solution in $31$ iterations. The $\alpha$ values vary a lot. In the beginning we have $\alpha=64$ and $\alpha=16$, because the algorithm tries to take a larger step in hopes of converging faster. In later iterations, we see both lower and larger values (compared to using "BA") for $\alpha$. The large values for $\alpha$ in the beginning also means we see a significant convergence in the beginning, but it is much slower after up until the end. The $\mu$-values are sometimes negative, especially in the beginning, meaning we are actually using steepest descent in the beginning. \\
Using the modified Newton method with "AF", we see the same $\alpha$-values and convergence. Like before, we instead observe for $\mu=1$, so the method utilizes the Hessian to converge to a solution. Again, there is no significant performance boost doing this over using steepest descent. Note that the Hessian is also heavy to compute, so it is not necessarily an advantage to try and use it.

\end{document}
